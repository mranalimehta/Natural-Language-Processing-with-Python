{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196-S1-2019 Assessment 1  \n",
    "## Task - 1\n",
    "### Student Name: Mranali Mehta\n",
    "### Student ID: 29778271\n",
    "Date: 14/04/2018\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "##### Libraries used :\n",
    "re : for regular expression for extracting the data.\n",
    "\n",
    "json : for creating json document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the task-1\n",
    "The task is about extracting the unit information from a text file which is basically a scrapped web-page containing html tags.\n",
    "\n",
    "<li>The task specify to extract the unit_id, unit_title, synopsis, requirements, pre-requisites, co-requisites, outcomes and chief examiner of the unit and store this information in two different formats: json and xml.</li>\n",
    "<li>To accomplish the task, the text file is read and saved as a list of tokens in a variable</li>\n",
    "<li>The loop is used to iterate over the list. Some flag variables are used for the puurspose of error handling and keep count of certain lines.</li>\n",
    "<li>While the loop iterate over the list of token, one by one the each of the information is extracted using the regular expression.</li>\n",
    "<li>While the unit information are extracted we also need to store them in a proper format,so during extracting the information from the text file's tokens following things are performed in parallel :</li>\n",
    "<ol>The information is extracted from the text.</ol>\n",
    "<ol>Once the information is in variable, it is written to an xml file with proper tags.</ol>\n",
    "<ol>also it is stored n the dictionary at the same time which can be used later to dump into a json file.</ol>\n",
    "\n",
    "The process used is pretty straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the required andallowed libraries\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Open the input text file to read the content\n",
    "text_file = open(\"29778271.txt\", 'r')\n",
    "textfile_tokens = text_file.readlines()\n",
    "line = 0\n",
    "unit = 0\n",
    "\n",
    "# Open the output xml file to write into it & initialize the file with initial tags and count the line number.\n",
    "# I have done counting of line number because of an error occuring at certain tag. In order to handle that error it was required.\n",
    "xml_output = open(\"29778271.xml\",'w')\n",
    "xml_output.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\")\n",
    "line += 1\n",
    "xml_output.write(\"<units>\\n\")\n",
    "line += 1\n",
    "\n",
    "# Dictionary to hold extracted data are initialised for forming the JSON file later in the code.\n",
    "main_dictionary = {}\n",
    "unit_dictionary = {}\n",
    "unit_details = []\n",
    "\n",
    "#Unit code & title : The regular expression used for extracting the unit code\n",
    "unitcode_regex = r\"(<h1) (class=\\\"[a-z]+_[a-z]+\\\"><span) (class=\\\"unitcode\\\">)([A-Z]{3}[0-9]{4})(</span>)\"\n",
    "\n",
    "#Chief Examiner : The regex used for extracting chief examiner\n",
    "ch_ex_regex = r\"<p class=\\\"hbk-highlight-heading\\\">Chief examiner\\(s\\)</p>\"\n",
    "chex_name_regex = r\"(<a href=\\\"http\\://staffsearch\\.monash\\.edu/\\?name=)(.*)(\\\">)\"\n",
    "coordinator_regex = r\"<p class=\\\"hbk-highlight-heading\\\">Coordinator\\(s\\)</p>\"\n",
    "\n",
    "# Assessment/Requirements : The regex used for extracting assessment or requirements.\n",
    "assess_regex = r\"<h2 class=\\\"hbk-heading\\\">Assessment</h2>\"\n",
    "require_ptag_regex = r\"(<p>)(.*)(</p>)\"\n",
    "require_litag_regex = r\"(<li>)(.*)(</li>)\"\n",
    "workload_regex = r\"<h2 class=\\\"hbk-heading\\\">Workload requirements</h2>\"\n",
    "last_tag_regex = r\"<p class=\\\"visuallyhidden\\\" id=\\\"breadcrumb__label\\\">You are here:</p>\"\n",
    "\n",
    "# Synopsis and Outcomes : regex for synopsis and outcomes \n",
    "synop_regex = r\"<h2 class=\\\"hbk-heading\\\">Synopsis</h2>\"\n",
    "synop_ptag_regex = r\"(<p>)(.*)(</p>)\"\n",
    "synop_litag_regex = r\"(<li>)(.*)(</li>)\"\n",
    "outcome_regex = r\"<h2 class=\\\"hbk-heading\\\">Outcomes</h2>\"\n",
    "\n",
    "# Prerequisites & Corequisites : Regex for pre-requisites and co-requisites.\n",
    "prereq_regex = r\"<p class=\\\"hbk-preamble-heading\\\">Prerequisites</p>\"\n",
    "pre_regex = r\"[A-Z]{3}[0-9]{4}\"\n",
    "prohibition_regex = r\"<p class=\\\"hbk-preamble-heading\\\">Prohibitions</p>\"\n",
    "\n",
    "for index,token in enumerate(textfile_tokens):\n",
    "    \n",
    "    #Saving one unit details and initialising for the first time\n",
    "    if (re.search(r\"<div class=\\\"hbk-banner-box\\\">\", token)):\n",
    "        if unit == 0:\n",
    "            xml_output.write(\"\\t<unit>\\n\")\n",
    "            line += 1\n",
    "            unit_dict = {}\n",
    "            unit += 1\n",
    "        else:\n",
    "            unit_details.append(unit_dict)\n",
    "                        \n",
    "    #Unit code and Unit title : if condition to extract and store unit_id and title in xml and dictionary.\n",
    "    if re.search(unitcode_regex, token):\n",
    "        u_id = re.search(unitcode_regex, token).group(4)\n",
    "        s_ind = re.search(unitcode_regex, token).end()\n",
    "        e_ind = token.index(\"class=\\\"hbk-archive-only\\\">\")\n",
    "        title = token[s_ind : e_ind-6]\n",
    "        if line == 1189:\n",
    "            xml_output.write(\"\\t</unit>\\n\")\n",
    "        else:\n",
    "            xml_output.write(\"\\t</unit>\\n\")\n",
    "        line += 1\n",
    "        xml_output.write(\"\\t<unit id='\" + str(u_id) + \"'>\\n\")\n",
    "        line += 1\n",
    "        xml_output.write(\"\\t\\t<title>\"+ title + \"</title>\\n\")\n",
    "        line += 1\n",
    "        unit_dict[\"@id\"] = u_id\n",
    "        unit_dict[\"title\"] = title\n",
    "        \n",
    "    # Synopsis :if condition to extract and store synopsis in xml and dictionary.\n",
    "    synop_ls = []\n",
    "    if re.search(synop_regex, token):\n",
    "        local_index = index\n",
    "        local_token = textfile_tokens[local_index]\n",
    "        while not(re.search(outcome_regex,local_token)):\n",
    "            if re.search(synop_ptag_regex, local_token):\n",
    "                synop = re.search(synop_ptag_regex,local_token).group(2)\n",
    "                lr = r\"(</p><p>)(.*)(<a) (.*)(</a>)\"\n",
    "                if re.search(lr,synop):\n",
    "                    r = re.search(lr,synop).group()\n",
    "                    synop = synop.replace(r,'')\n",
    "                synop_ls.append(synop)\n",
    "                xml_output.write(\"\\t\\t<synopsis>\" + synop + \"</synopsis>\\n\")\n",
    "                line += 1\n",
    "            if re.search(synop_litag_regex, local_token):\n",
    "                synop = re.search(synop_litag_regex,local_token).group(2)\n",
    "                lr = r\"(</p><p>)(.*)(<a) (.*)(</a>)\"\n",
    "                if re.search(lr,synop):\n",
    "                    r = re.search(lr,synop).group()\n",
    "                    synop = synop.replace(r,'')\n",
    "                synop_ls.append(synop)\n",
    "                xml_output.write(\"\\t\\t<synopsis>\" + synop + \"</synopsis>\\n\")\n",
    "                line += 1\n",
    "            local_index += 1\n",
    "            local_token = textfile_tokens[local_index]\n",
    "        unit_dict[\"synopsis\"] = synop_ls\n",
    "\n",
    "    #Prerequisite : if condition to extract and store prerequisites and corequisites combines in xml and dictionary.\n",
    "    pre_requistics = {}\n",
    "    pre_ls = []\n",
    "    if re.search(prereq_regex, token):\n",
    "        local_index = index\n",
    "        local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t<pre_requistics>\\n\")\n",
    "        line += 1\n",
    "        while not(re.search(synop_regex, local_token)):\n",
    "            if re.search(prohibition_regex, token):\n",
    "                break\n",
    "            if re.search(pre_regex, local_token):\n",
    "                prereq = re.search(pre_regex, local_token).group()\n",
    "                if prereq not in pre_ls:\n",
    "                    pre_ls.append(prereq)\n",
    "                    xml_output.write(\"\\t\\t\\t<pre_requistic>\" + prereq + \"</pre_requistic>\\n\")\n",
    "                    line += 1\n",
    "            local_index += 1\n",
    "            local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t</pre_requistics>\\n\")\n",
    "        line += 1\n",
    "        pre_requistics[\"pre_requistic\"] = pre_ls\n",
    "        unit_dict[\"pre_requistics\"] = pre_requistics\n",
    "        \n",
    "    #Prohibition : if condition to extract and store Prohibitions in xml and dictionary.\n",
    "    prohibitions = []\n",
    "    if re.search(prohibition_regex, token):\n",
    "        flag = False\n",
    "        local_index = index\n",
    "        local_token = textfile_tokens[local_index]\n",
    "        while not(re.search(synop_regex, local_token)):\n",
    "            if re.search(pre_regex, local_token):\n",
    "                    prereq = re.search(pre_regex, local_token).group()\n",
    "                    if prereq not in prohibitions:\n",
    "                        prohibitions.append(prereq)\n",
    "                        xml_output.write(\"\\t\\t<prohibitions>\" + prereq + \"</prohibitions>\\n\")\n",
    "                        unit_dict[\"prohibitions\"] = prereq\n",
    "                        line += 1\n",
    "                        flag =True\n",
    "            local_index += 1\n",
    "            local_token = textfile_tokens[local_index]\n",
    "        if flag is False:\n",
    "                prohibitions.append(\"NA\")\n",
    "                xml_output.write(\"\\t\\t<prohibitions>\" + \"NA\" + \"</prohibitions>\\n\")\n",
    "                unit_dict[\"prohibitions\"] = \"NA\"\n",
    "                line += 1\n",
    "\n",
    "    # Assessment/Requirements : if condition to extract and store requirements in xml and dictionary.\n",
    "    requirements ={}\n",
    "    assessment = []\n",
    "    flag = False\n",
    "    if re.search(assess_regex, token):\n",
    "        flag = True\n",
    "        local_index = index\n",
    "        local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t<requirements>\\n\")\n",
    "        line += 1\n",
    "        while flag:\n",
    "            if re.search(require_ptag_regex, local_token):\n",
    "                assess = re.search(require_ptag_regex, local_token).group(2)\n",
    "                l = len(\"</p><p>\")\n",
    "                i = 0\n",
    "                i = assess.find(\"</p><p>\")\n",
    "                assess = assess[0:i]+assess[i+l:]\n",
    "                if assess.find(\"<a href=\") is (-1):\n",
    "                    assessment.append(assess)\n",
    "                    xml_output.write(\"\\t\\t\\t<requirement>\" + assess + \"</requirement>\\n\")\n",
    "                    line += 1\n",
    "            if re.search(require_litag_regex, local_token):\n",
    "                assess = re.search(require_litag_regex, local_token).group(2)\n",
    "                assessment.append(assess)\n",
    "                xml_output.write(\"\\t\\t\\t<requirement>\" + assess + \"</requirement>\\n\")\n",
    "                line += 1\n",
    "            elif (re.search(workload_regex, local_token)):\n",
    "                flag = False\n",
    "            elif (re.search(last_tag_regex, local_token)):\n",
    "                flag = False\n",
    "            local_index += 1\n",
    "            local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t</requirements>\\n\")\n",
    "        line += 1\n",
    "        requirements[\"requirement\"] = assessment\n",
    "        unit_dict[\"requirements\"] = requirements\n",
    "        \n",
    "    # Outcomes : if condition to extract and store Outcomes in xml and dictionary.\n",
    "    outcomes = {}\n",
    "    out_ls = []\n",
    "    if re.search(outcome_regex, token):\n",
    "        local_index = index\n",
    "        local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t<outcomes>\\n\")\n",
    "        line += 1\n",
    "        while not(re.search(assess_regex,local_token)):\n",
    "            if re.search(synop_ptag_regex, local_token):\n",
    "                outcome = re.search(synop_ptag_regex,local_token).group(2)\n",
    "                lr = r\"(</p><p>)(.*)(<a) (.*)(</a>)\"\n",
    "                if re.search(lr,outcome):\n",
    "                    r = re.search(lr,outcome).group()\n",
    "                    outcome = outcome.replace(r,'')\n",
    "                out_ls.append(outcome)\n",
    "                xml_output.write(\"\\t\\t\\t<outcome>\" + outcome + \"</outcome>\\n\")\n",
    "                line += 1\n",
    "            if re.search(synop_litag_regex, local_token):\n",
    "                outcome = re.search(synop_litag_regex,local_token).group(2)\n",
    "                lr = r\"(</p><p>)(.*)(<a) (.*)(</a>)\"\n",
    "                if re.search(lr,outcome):\n",
    "                    r = re.search(lr,outcome).group()\n",
    "                    outcome = outcome.replace(r,'')\n",
    "                out_ls.append(outcome)\n",
    "                xml_output.write(\"\\t\\t\\t<outcome>\" + outcome + \"</outcome>\\n\")\n",
    "                line += 1\n",
    "            local_index += 1\n",
    "            local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t</outcomes>\\n\")\n",
    "        line += 1\n",
    "        outcomes[\"outcome\"] = out_ls\n",
    "        unit_dict[\"outcomes\"] = outcomes\n",
    "        \n",
    "    #Chief Examiner : if condition to extract and store Chief Examiner in xml and dictionary.\n",
    "    chief_examiners = {}\n",
    "    chief_exam = []\n",
    "    if re.search(ch_ex_regex, token):\n",
    "        local_index = index\n",
    "        local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t<chief_examiners>\\n\")\n",
    "        line += 1\n",
    "        while not(re.search(coordinator_regex, local_token)):\n",
    "            if re.search(chex_name_regex,local_token):\n",
    "                ch_name = re.search(chex_name_regex,local_token).group(2)\n",
    "                chief_exam.append(ch_name)\n",
    "                xml_output.write(\"\\t\\t\\t<chief_examiner>\" + ch_name + \"</chief_examiner>\\n\")\n",
    "                line += 1\n",
    "            elif re.search(\"<!-- TBA -->TBA\", local_token):\n",
    "                tba = re.search(\"<!-- TBA -->TBA\", local_token).group()\n",
    "                chief_exam.append(tba)\n",
    "                xml_output.write(\"\\t\\t\\t<chief_examiner>\" + \"TBA\" + \"</chief_examiner>\\n\")\n",
    "                line += 1\n",
    "            local_index += 1\n",
    "            local_token = textfile_tokens[local_index]\n",
    "        xml_output.write(\"\\t\\t</chief_examiners>\\n\")\n",
    "        line += 1\n",
    "        chief_examiners[\"chief_examiner\"] = chief_exam\n",
    "        unit_dict[\"chief_examiners\"] = chief_examiners\n",
    "        \n",
    "# Finally when all the data is extracted of all the units they are stored in list of dictionaries named : unit_details.\n",
    "# So in last this list is stored in a dictionary named unit_dictionary with key as 'unit' and finally the unit_dictionary is \n",
    "# stored in main_dictionary with key as 'units'. This is done so that this format can directly be dumped to json to create the \n",
    "# required format.\n",
    "unit_dictionary[\"unit\"] = unit_details\n",
    "main_dictionary[\"units\"] = unit_dictionary        \n",
    "\n",
    "# finally closing the tags.\n",
    "xml_output.write(\"\\t</unit>\\n\")\n",
    "xml_output.write(\"</units>\")\n",
    "line += 1\n",
    "\n",
    "# closing the connection with files.\n",
    "xml_output.close()\n",
    "text_file.close()\n",
    "\n",
    "# JSON file : Creating the json file by using the main_dictionary containing all the units.\n",
    "extracted_json = open(\"29778271.json\",'w')\n",
    "json.dump(main_dictionary, extracted_json, indent = 2)\n",
    "extracted_json.close()\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
