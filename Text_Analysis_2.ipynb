{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196-S1-2019 Assessment 1\n",
    "## Task 2\n",
    "### Student Name: Mranali Mehta\n",
    "### Student ID: 29778271\n",
    "Date: 14/04/2018\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "##### Libraries used :\n",
    "re : for regular expression for tokenizing the text.\n",
    "\n",
    "nltk.collocations : used for creating bigrams\n",
    "\n",
    "itertools : for iterating over the dictionary\n",
    "\n",
    "MWETokenizer : To tokenise the bigrams \n",
    "\n",
    "PorterStemmer : To stem the token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries required for performing the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from __future__ import division\n",
    "from nltk.probability import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Open the files required. \n",
    "###### Initialize the data structure to be used, regular exoression to be used.\n",
    "###### Open the input files to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open(\"29778271.txt\",'r')\n",
    "\n",
    "text_in_file = input_file.readlines()\n",
    "\n",
    "unit_info_dict = {}\n",
    "\n",
    "unit_regex = r\"[A-Z]{3}[0-9]{4}\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean the data after reading it from the text file & Normalise the first letter of sentence. \n",
    "###### Cleaning is performed as:\n",
    "-> Removing the '\\n' from the end of tokens.\n",
    "\n",
    "-> Removing the special characters or brackets from the tokens.\n",
    "\n",
    "-> Replace the unit code occuring between the text which can create ambiguity.\n",
    "\n",
    "-> Normalise the first letter in the token at the start of each sentence, leaving the upper case token coming inside the text.\n",
    "\n",
    "-> Also remove the alphanumeric tokens.\n",
    "\n",
    "Finally store the list of cleaned and normalised tokens in the form of list into a dictionary whose key is unit_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Cleaning the unit info in the text file for further pre-processing\n",
    "## Normalising the first letter in each sentence.\n",
    "\n",
    "for index,line in enumerate(text_in_file):\n",
    "    unit_info_ls = []\n",
    "    count = 0\n",
    "    if re.search(unit_regex,line):\n",
    "        unit_id = re.search(unit_regex,line).group()\n",
    "        if unit_id[-1] == '\\n':\n",
    "            unit_id = unit_id.replace('\\n','')\n",
    "        i = index + 1\n",
    "        token = text_in_file[i]\n",
    "        while count < 3:\n",
    "            if token == '\\n':\n",
    "                count += 1\n",
    "            else:\n",
    "                if token[0] == \"[\":\n",
    "                    token = token.replace(\"[\",\"\")\n",
    "                    if token[0] == \"\\'\":\n",
    "                        token = token.replace(\"\\'\",\"\")\n",
    "                if token[-1] == \"\\n\":\n",
    "                    token = token.replace(\"\\n\",\"\")\n",
    "                if token[-1] == \"]\":\n",
    "                    token = token.replace(\"]\",\"\")\n",
    "                if re.search(r\"[A-Z]{3}[0-9]{4}\",token):\n",
    "                    token = token.replace(re.search(r\"[A-Z]{3}[0-9]{4}\",token).group(),\" \")\n",
    "                if token[0].isupper():\n",
    "                    token = token.replace(token[0],token[0].lower())\n",
    "                if not token.isalnum():\n",
    "                    unit_info_ls.append(token)\n",
    "            i += 1\n",
    "            token = text_in_file[i]\n",
    "        unit_info_dict[unit_id] = unit_info_ls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenising the cleaned text.\n",
    "\n",
    "-> Make use of the regex given and tokenize the text based on that. \n",
    "\n",
    "-> Tokenisation is done unit wise. Information of one unit is tokenized at a time and stored in one list.\n",
    "\n",
    "-> Then this list is added to the dictionary with key as its unit ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text\n",
    "reg = r\"\\w+(?:[-']\\w+)?\"\n",
    "for key in unit_info_dict.keys():\n",
    "    ui =[]\n",
    "    local_str = \"\"\n",
    "    for each_line in unit_info_dict[key]:\n",
    "        local_str = local_str + \" \" + each_line \n",
    "    ui = re.findall(reg,local_str)\n",
    "    unit_info_dict[key] = ui\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing the stop words from the tokens list.\n",
    "\n",
    "Read the stopwords provided in the text file. Compare them and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the stop words and removing the \\n from each stop word.        \n",
    "stopwords = open(\"stopwords_en.txt\",'r')\n",
    "stopwords_list = stopwords.readlines()\n",
    "stop_words = []\n",
    "for stopword in stopwords_list:\n",
    "    if stopword[-1] == '\\n':\n",
    "        stopword = stopword.replace(\"\\n\",\"\")\n",
    "        stop_words.append(stopword)\n",
    "\n",
    "# removing stop words\n",
    "for key,value in unit_info_dict.items():\n",
    "    filtered_unit_info = []\n",
    "    for token in value:\n",
    "        if token not in stop_words:\n",
    "            filtered_unit_info.append(token)\n",
    "    unit_info_dict[key] = filtered_unit_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove tokens with length less than 3\n",
    "\n",
    "Look for tokens with length less than 3 and remove them.\n",
    "\n",
    "Also, I removed 'the' and 'this' separately as they were not removed from the tokens and were occuring so many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the tokens from the unit_info_dict which is less than 3 in length\n",
    "for key,value in unit_info_dict.items():\n",
    "    for token in value:\n",
    "        if len(token) < 3:\n",
    "            value.remove(token)\n",
    "            \n",
    "# Removing the, this from the unit_info\n",
    "for key,value in unit_info_dict.items():\n",
    "    for token in value:\n",
    "        if token in ['the','this','The','This']:\n",
    "            value.remove(token)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find the biagrams, stem the tokens and then tokenize the biagrams.\n",
    "\n",
    "First find out the top 200 bigrams.\n",
    "\n",
    "Tokenise them nd then stem the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the bigrams \n",
    "all_tokens = list(chain.from_iterable(unit_info_dict.values()))\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "bigram_finder.apply_freq_filter(10)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "all_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "# re-tokenising the bigrams after stemming the tokens\n",
    "unit_info_bigram = {}\n",
    "mwetokenizer = MWETokenizer(all_bigrams)\n",
    "for key,value in unit_info_dict.items():\n",
    "    value_colloc = mwetokenizer.tokenize(value)\n",
    "    unit_info_bigram[key] = value_colloc\n",
    "    \n",
    "unit_info_dict = unit_info_bigram\n",
    "    \n",
    "# Stemming the unit_info\n",
    "stemmer = PorterStemmer()\n",
    "for key,value in unit_info_dict.items():\n",
    "    stem_ls = []\n",
    "    for w in value:\n",
    "        if w[0].isupper():\n",
    "            stem_ls.append(w)\n",
    "        else:\n",
    "            stem_ls.append(stemmer.stem(w))\n",
    "    unit_info_dict[key] = stem_ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating the dictionary\n",
    "\n",
    "Iterate through the list of values in the dictionary and form a list and then pass it to set to generate unique list of words called as dictionary.\n",
    "\n",
    "Remove the words which occur in more than 95% of the units or less than 5% of the units.\n",
    "\n",
    "Finally, store it in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocab-dictionary\n",
    "words = list(chain.from_iterable(unit_info_dict.values()))\n",
    "vocab = set(words)\n",
    "\n",
    "# Removing words with frequency more than 95% and less than 5% from the dictionary\n",
    "less_than_5 = []\n",
    "more_than_95 = []\n",
    "vocab_ls = []\n",
    "\n",
    "for each in vocab:\n",
    "    word_count = 0\n",
    "    for value in unit_info_dict.values():\n",
    "        if each in value:\n",
    "            word_count += 1\n",
    "    if word_count < 10:\n",
    "        less_than_5.append(each)\n",
    "    elif word_count > 190:\n",
    "        more_than_95.append(each)\n",
    "    else:\n",
    "        vocab_ls.append(each)\n",
    "\n",
    "# After removing rare tokens and most frequent tokens from the vocab we can finally add all the 200 bigrams to the vocab\n",
    "bigrams_ls = list(chain.from_iterable(unit_info_bigram.values()))\n",
    "bigrams = set(bigrams_ls)\n",
    "vocab_ls.sort()\n",
    "vocab = set(vocab_ls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Write the words to the vocab.txt with indexing.\n",
    "## Add the top 200 bigrams in the vocab.\n",
    "\n",
    "Finally write the vocabulary generated to the text file with proper indexing.\n",
    "\n",
    "Then add the 200 bigrams to the vocab.txt if they do not exist in vocab so as to avoid the duplication of the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to vocab.txt\n",
    "vocab_output = open(\"29778271_vocab.txt\",'w')\n",
    "index = 0\n",
    "for unique in vocab_ls:\n",
    "    vocab_output.write(unique + \":\" + str(index) + \"\\n\")\n",
    "    index += 1\n",
    "for bigram in bigrams:\n",
    "    if \"_\" in bigram:\n",
    "        if bigram not in vocab_ls:\n",
    "            vocab_output.write(bigram + \":\" + str(index) + \"\\n\")\n",
    "            index += 1\n",
    "            \n",
    "vocab_output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate the countVec.\n",
    "\n",
    "The last step to generate the space vector model.\n",
    "\n",
    "Read the words, find the frequency distribution of each word per unit and write the corresponding index for that word with its frequency into a text file.\n",
    "\n",
    "Repeat the same thing for all the units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVec Generation:\n",
    "\n",
    "count_vec_output = open(\"29778271_countVec.txt\",'w')\n",
    "\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for w in vocab_ls:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "for key,val in unit_info_dict.items():\n",
    "    count_vec_output.write(key + ',')\n",
    "    word_index = []\n",
    "    for word in val:\n",
    "        if word in vocab_dict.keys():\n",
    "            word_index.append(vocab_dict[word])\n",
    "    for k, v in FreqDist(word_index).items():\n",
    "        count_vec_output.write(\"{}:{} \".format(k,v) + ',')\n",
    "    count_vec_output.write('\\n')\n",
    "count_vec_output.close()\n",
    "\n",
    "# finally close the input file as well.\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
